WEBVTT

1
00:00:09.054 --> 00:00:11.732
So now we're actually going to
write our real web crawler, and

2
00:00:11.732 --> 00:00:14.240
we're going to actually parse the HTML.

3
00:00:14.240 --> 00:00:18.070
And most of the time I show
you the hard way first, and

4
00:00:18.070 --> 00:00:20.180
then I show you the easy way second.

5
00:00:20.180 --> 00:00:21.899
But in this one we're not
gong to bother with that.

6
00:00:21.899 --> 00:00:23.580
I'm like oh, look how much easier it is.

7
00:00:23.580 --> 00:00:26.940
No, we're just going to
start with the easy way.

8
00:00:26.940 --> 00:00:32.330
The problem is if you look at HTML,
just here's some sample HTML,

9
00:00:32.330 --> 00:00:35.428
you can break this stuff on,
it doesn't have to all be on one line.

10
00:00:35.428 --> 00:00:39.890
Well, you might write HTML, makes really
good sense, and so it could be ugly.

11
00:00:39.890 --> 00:00:42.250
And you don't even want to
know the rules of HTML.

12
00:00:42.250 --> 00:00:45.810
It's bad enough to know the rules of
HTML when you're writing web pages,

13
00:00:45.810 --> 00:00:49.470
let alone trying to read
someone else's crazy HTML.

14
00:00:49.470 --> 00:00:52.020
And as HTML starts coming
from applications,

15
00:00:52.020 --> 00:00:54.232
if you're going to learn how to do
web development like in PHP,

16
00:00:54.232 --> 00:00:58.820
when HTML that comes out of these programs
it's sometimes really ugly with blanks and

17
00:00:58.820 --> 00:01:02.200
newlines and all kinds of crap.

18
00:01:02.200 --> 00:01:03.570
And it's all legit, right?

19
00:01:03.570 --> 00:01:07.330
And the browsers are really
smart about compensating for

20
00:01:07.330 --> 00:01:10.930
ugly HTML that's valid but ugly HTML.

21
00:01:12.890 --> 00:01:16.621
And slowly but surely you could
write a parser for HTML.

22
00:01:16.621 --> 00:01:19.229
But then you would find out like,
oh, somebody did this page, and

23
00:01:19.229 --> 00:01:21.809
they used a single quote or
they forgot the quotes, or whatever.

24
00:01:22.880 --> 00:01:26.570
Well someone's done that already, and they
wrote this thing called Beautiful Soup.

25
00:01:26.570 --> 00:01:31.400
Beautiful Soup is sort of a play on the
children's book called Stone Soup where

26
00:01:31.400 --> 00:01:35.361
you kind of throw a bunch of junky
things in and it turns out great.

27
00:01:35.361 --> 00:01:40.203
Well, I think that's what they mean
when they call it Beautiful Soup,

28
00:01:40.203 --> 00:01:41.962
that HTML's all junky.

29
00:01:41.962 --> 00:01:45.165
And if you throw all the crappy
HTML into Beautiful Soup,

30
00:01:45.165 --> 00:01:50.620
then what comes out of Beautiful Soup is
wonderful and delicious parsable HTML.

31
00:01:50.620 --> 00:01:54.530
So this is also a good time to talk
about Python 2 versus Python 3.

32
00:01:54.530 --> 00:01:57.309
This class of course for
now is in Python 2, and

33
00:01:57.309 --> 00:02:01.284
so what I'm going to show you is how
to use Beautiful Soup in Python 2.

34
00:02:01.284 --> 00:02:02.747
The concepts are very similar.

35
00:02:02.747 --> 00:02:05.771
The installation's a little different for
Python 3, and

36
00:02:05.771 --> 00:02:08.685
Beautiful Soup is ported both
to Python 2 and Python 3.

37
00:02:08.685 --> 00:02:10.450
So you can use this library.

38
00:02:10.450 --> 00:02:12.020
It's a great library.

39
00:02:12.020 --> 00:02:17.020
Now if you're doing Python 2,
which is our Python, you can

40
00:02:17.020 --> 00:02:21.370
download the file BeautifulSoup.py, put it
in the same folder as your Python code.

41
00:02:21.370 --> 00:02:23.950
There are alternative ways to put this in.

42
00:02:23.950 --> 00:02:25.356
But this is the crude way.

43
00:02:25.356 --> 00:02:29.014
And if you're having trouble finding
BeautifulSoup.py, it's on my Python for

44
00:02:29.014 --> 00:02:30.080
Informatics website.

45
00:02:30.080 --> 00:02:34.960
And you can download it from there, either
that or from the original crummy.com.

46
00:02:34.960 --> 00:02:37.600
So here is a easy way to do it.

47
00:02:37.600 --> 00:02:40.860
I mean,
literally this is the whole program.

48
00:02:40.860 --> 00:02:43.750
What we're going to do in this program
is we're going to retrieve a web page,

49
00:02:44.860 --> 00:02:47.010
and we are going to parse the web page, and

50
00:02:47.010 --> 00:02:50.950
we're going to look at all the anchor
tags and print out the hrefs.

51
00:02:50.950 --> 00:02:54.375
That's it, this is the whole thing,
thanks to Beautiful Soup.

52
00:02:54.375 --> 00:02:57.835
We're not doing any regular expressions,
any find operations, nothing,

53
00:02:57.835 --> 00:03:00.475
because Beautiful Soup does it all.

54
00:03:00.475 --> 00:03:02.545
So if we look at this,
we have imports at the top.

55
00:03:02.545 --> 00:03:03.815
We import the urllib.

56
00:03:03.815 --> 00:03:06.020
So that's how we're going to
actually read the HTML data and

57
00:03:06.020 --> 00:03:07.825
retrieve it into our program.

58
00:03:07.825 --> 00:03:09.575
Then we're going to import the library,

59
00:03:09.575 --> 00:03:12.165
all the routines that are in
the BeautifulSoup.py file.

60
00:03:12.165 --> 00:03:13.740
That's kind of what that says.

61
00:03:13.740 --> 00:03:14.692
We're going to use raw_input.

62
00:03:14.692 --> 00:03:18.390
That's familiar to us,
how we prompt for the name of the URL.

63
00:03:18.390 --> 00:03:22.240
Then we're going to call urllib.urlopen
with the URL as parameter.

64
00:03:22.240 --> 00:03:25.050
Now the one thing we haven't done before is
we're just going to call the read method on

65
00:03:25.050 --> 00:03:30.310
that, and what that means says
read it all, newlines and all.

66
00:03:30.310 --> 00:03:31.400
We've done this before. And so

67
00:03:31.400 --> 00:03:35.390
it gives us all the lines in a single
call with the newlines intact.

68
00:03:36.470 --> 00:03:41.265
But if you think about it, well, in this
case, we're not going to do any splitting,

69
00:03:41.265 --> 00:03:42.990
but it's okay to read it all.

70
00:03:42.990 --> 00:03:46.926
The pages shouldn't be all that long,
and so this not only opens the URL but

71
00:03:46.926 --> 00:03:47.590
reads it.

72
00:03:47.590 --> 00:03:51.548
So we've collapsed that down
to a single line, read it all.

73
00:03:51.548 --> 00:03:53.674
So what we get, this is a string.

74
00:03:53.674 --> 00:03:55.900
I call it html, but it could be anything.

75
00:03:55.900 --> 00:03:59.570
html is a string, which is the entire
web page with less thans and

76
00:03:59.570 --> 00:04:00.930
greater thans and newlines.

77
00:04:00.930 --> 00:04:02.690
That's what that line does.

78
00:04:02.690 --> 00:04:05.670
Again with Python, collapses right
down to one line, really nice.

79
00:04:07.250 --> 00:04:12.174
So Beautiful Soup. We're going to call
Beautiful Soup, say munge this,

80
00:04:12.174 --> 00:04:14.540
read this, figure this out.

81
00:04:14.540 --> 00:04:17.880
Here's our string that we read,
and then make sense of it and

82
00:04:17.880 --> 00:04:19.390
give us back this soup object.

83
00:04:20.540 --> 00:04:26.020
So soup is neither a string nor a boolean
or a dictionary, it's lots of stuff.

84
00:04:26.020 --> 00:04:31.930
It is the parsed HTML data, and
then you can ask soup questions, okay?

85
00:04:31.930 --> 00:04:36.130
So this is a soup object that's there for
you to make use of.

86
00:04:36.130 --> 00:04:43.130
And so we can retrieve a list of tags
by saying soup and pass the a tag.

87
00:04:43.130 --> 00:04:45.429
And what that really is looking for

88
00:04:45.429 --> 00:04:49.006
is things that look like
a dot dot dot slash, slash a.

89
00:04:49.006 --> 00:04:50.880
So that's what an anchor tag looks like.

90
00:04:50.880 --> 00:04:53.430
And what we're saying is,
find me all the tags.

91
00:04:53.430 --> 00:04:55.810
So don't find me p tags.

92
00:04:55.810 --> 00:04:58.000
Don't find me bold tags.

93
00:04:58.000 --> 00:05:01.020
Don't find me any of that stuff,
just give me the tags.

94
00:05:01.020 --> 00:05:04.630
And what we're really getting is this
data right here that's the tag itself.

95
00:05:05.640 --> 00:05:08.340
Okay? And so that is a list of tags.

96
00:05:08.340 --> 00:05:14.200
And so if this had one anchor tag,
this would be a list of one tag.

97
00:05:14.200 --> 00:05:16.630
If there were zero anchor tags,
we'd get no tags.

98
00:05:17.750 --> 00:05:22.490
This is a really smart lookup that
reads the entire HTML document and

99
00:05:22.490 --> 00:05:24.760
however many tags are there,
it gives them to us. Okay?

100
00:05:25.980 --> 00:05:28.860
Ignores all these other tags and
just finds those tags.

101
00:05:28.860 --> 00:05:31.050
So that is a list of tags.

102
00:05:31.050 --> 00:05:32.640
It's kind of like a dictionary.

103
00:05:32.640 --> 00:05:36.788
Because if you look at what the tag says,

104
00:05:36.788 --> 00:05:42.897
it's like a href = quote,
[NOISE], double quote, [NOISE].

105
00:05:42.897 --> 00:05:45.997
And so,
these things here are called attributes.

106
00:05:45.997 --> 00:05:49.234
And so when we grab the tag,
we get all of this, and

107
00:05:49.234 --> 00:05:54.710
then it parses these things as well, hrefs
as key-value pairs in like a dictionary.

108
00:05:54.710 --> 00:05:58.240
And so what we can do is we can look
at all the tags in the document.

109
00:05:58.240 --> 00:06:03.580
For tag in tags, that's looping through
all the anchor tags in the document.

110
00:06:03.580 --> 00:06:07.087
How many a's there are in this document,
da, da, da, da, da, loop through them all,

111
00:06:07.087 --> 00:06:08.885
ignoring everything else in the document.

112
00:06:08.885 --> 00:06:12.030
And then give us the href value for
each of those tags.

113
00:06:12.030 --> 00:06:14.980
And we're going to use a get, and
we're going to have a default value of None.

114
00:06:14.980 --> 00:06:16.810
So that we can, if there is no href,

115
00:06:16.810 --> 00:06:20.460
because it's possible to do
an anchor tag with no href.

116
00:06:20.460 --> 00:06:25.560
And that, literally, is going to retrieve the
document, and show us all the anchor tags.

117
00:06:25.560 --> 00:06:28.700
Seriously, that's it, that's it.

118
00:06:28.700 --> 00:06:33.030
So let's just run that. Come back.

119
00:06:33.030 --> 00:06:36.281
What's the name of that file again?

120
00:06:36.281 --> 00:06:38.127
urllinks.py, let's take a look at it.

121
00:06:44.083 --> 00:06:45.429
That's it.

122
00:06:45.429 --> 00:06:51.050
Read a web page, any web page we type,
read it, use Beautiful Soup to parse it.

123
00:06:51.050 --> 00:06:55.409
Look for all the anchor tags,
loop through the anchor tags, and

124
00:06:55.409 --> 00:06:57.356
print out the hrefs, okay?

125
00:06:57.356 --> 00:07:03.718
python urllinks.py.

126
00:07:03.718 --> 00:07:06.576
So let's just start with that other file,

127
00:07:06.576 --> 00:07:16.742
http://www.dr-chuck.com/page1.htm.

128
00:07:16.742 --> 00:07:18.181
And we know what this looks like.

129
00:07:18.181 --> 00:07:21.480
And you could put print statements to
print the HTML out, but I don't have that.

130
00:07:21.480 --> 00:07:26.450
It's going to retrieve it with a .read, pull
all the text, pass it into Beautiful Soup.

131
00:07:26.450 --> 00:07:30.754
And then loop through and print out
all the hrefs of all the anchor tags.

132
00:07:30.754 --> 00:07:32.430
End of story, right?

133
00:07:32.430 --> 00:07:36.190
Did it. That's just seven lines of code.

134
00:07:36.190 --> 00:07:40.050
And you just did all that work, because
we're depending on the urllib library and

135
00:07:40.050 --> 00:07:41.670
the Beautiful Soup library.

136
00:07:41.670 --> 00:07:44.100
But we don't have to just
have a page with one link.

137
00:07:44.100 --> 00:07:52.205
We could say, http://www.dr-chuck.com.

138
00:07:52.205 --> 00:07:54.585
See what happens there.

139
00:07:54.585 --> 00:07:56.830
See how many links I've got on that page.

140
00:07:56.830 --> 00:08:01.050
So there we go.
That is all the links on that page, right?

141
00:08:01.050 --> 00:08:02.050
It read all that stuff.

142
00:08:03.930 --> 00:08:11.000
So right here, dr-chuck.com, these from
here down are all the links in that page.

143
00:08:11.270 --> 00:08:15.500
Now you can see how easy it is to
eventually write a web scraper,

144
00:08:15.500 --> 00:08:19.930
which we will do later in
the book basically. Okay?

145
00:08:19.930 --> 00:08:21.210
And so that's it.

146
00:08:21.210 --> 00:08:24.350
That, and [LAUGH] I guess that's it.

147
00:08:25.410 --> 00:08:30.918
That's the simple essence of using
[LAUGH] really powerful Python libraries

148
00:08:30.918 --> 00:08:35.908
both built in with urllib and
with Beautiful Soup to both retrieve and

149
00:08:35.908 --> 00:08:39.539
parse these things in just
a couple lines of code.

150
00:08:39.539 --> 00:08:43.374
And I'm going to guess that the first
Google crawler was probably looked just

151
00:08:43.374 --> 00:08:44.420
about like this.

152
00:08:44.420 --> 00:08:46.577
These days it's super sophisticated code.

153
00:08:46.577 --> 00:08:51.142
But in the early days you could write
a crawler that used this essential bit of

154
00:08:51.142 --> 00:08:55.565
code that would retrieve these pages,
parse them using Beautiful Soup or

155
00:08:55.565 --> 00:08:57.200
a similar library.

156
00:08:57.200 --> 00:09:00.200
And then make a list of the next
things that have to be parsed and

157
00:09:00.200 --> 00:09:02.990
then just go and go and go and go.

158
00:09:02.990 --> 00:09:07.610
And eventually you sort of end up
with a full list of everything, and

159
00:09:07.610 --> 00:09:09.170
you too could have a search engine.

160
00:09:09.170 --> 00:09:11.850
But it's a little late now, because
someone already did a search engine.

161
00:09:13.530 --> 00:09:16.930
So if we take a look
at this whole chapter,

162
00:09:18.700 --> 00:09:23.700
the Internet and the Internet
protocols are very beautiful and

163
00:09:23.700 --> 00:09:28.050
very complex and knit us all together and
connect applications to applications.

164
00:09:29.610 --> 00:09:33.960
In a programming sense, the lowest that
we tend to talk to is the sockets,

165
00:09:33.960 --> 00:09:36.850
which are like,
make a phone call to this address,

166
00:09:36.850 --> 00:09:40.000
which is like make a phone call through
a phone number with an extension.

167
00:09:40.000 --> 00:09:44.500
The ports are the extensions that
allows us to talk to the web server,

168
00:09:44.500 --> 00:09:46.437
the mail server, the login server.

169
00:09:47.620 --> 00:09:50.090
So on one host you can
have multiple servers.

170
00:09:50.090 --> 00:09:51.530
And then once you talk to that server,

171
00:09:51.530 --> 00:09:53.336
you've got to know
the application protocol.

172
00:09:53.336 --> 00:09:57.350
And in this case we played with
the HTTP application protocol.

173
00:09:57.350 --> 00:09:59.454
We send these GET requests.

174
00:09:59.454 --> 00:10:02.467
And we've talked to the socket library.

175
00:10:02.467 --> 00:10:04.420
We've talked to the urllib library.

176
00:10:04.420 --> 00:10:07.660
And then we talked using
the Beautiful Soup library.

177
00:10:07.660 --> 00:10:12.330
And, when it's all said and done,
our programs end up relatively small.

178
00:10:12.330 --> 00:10:16.087
And Python has really good support for
talking across the network

179
00:10:16.087 --> 00:10:20.809
to these various applications, retrieving,
parsing, and making sense of it.

180
00:10:20.809 --> 00:10:24.720
Now, up next, we'll be talking to
things that are even more intelligent.

181
00:10:24.720 --> 00:10:27.478
Now HTML is not designed to transfer data.

182
00:10:27.478 --> 00:10:30.368
But using Beautiful Soup,
we can kind of pretend that it is data,

183
00:10:30.368 --> 00:10:32.950
and tear our stuff out
that we want to look at.

184
00:10:32.950 --> 00:10:37.310
But, up next we're going to talk to things
that actually want to talk with us, and

185
00:10:37.310 --> 00:10:42.190
give us data in an even better format
than HTML that makes it easier and

186
00:10:42.190 --> 00:10:45.920
more natural for
us to make sense of that data.

187
00:10:45.920 --> 00:10:47.253
So that's what we're going to do next.

188
00:10:47.253 --> 00:10:49.545
We're going to keep using
the network to retrieve data, but

189
00:10:49.545 --> 00:10:52.130
we're going to use very different
techniques in the next lecture.